# import torch
# from PIL import Image
# from transformers import BlipProcessor, BlipForConditionalGeneration

# # Load model and processor
# print("[INFO] Loading model...")
# model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
# processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
# model.eval()

# # Load image
# print("[INFO] Loading image...")
# image = Image.open("img.jpg").convert("RGB")
# inputs = processor(images=image, return_tensors="pt")

# # Only export the vision encoder for now (image -> embedding)
# print("[INFO] Extracting vision encoder...")
# vision_encoder = model.vision_model

# # Dummy input
# dummy_pixel_values = inputs['pixel_values']

# # Export vision encoder
# print("[INFO] Exporting vision encoder to ONNX...")
# torch.onnx.export(
#     vision_encoder,
#     (dummy_pixel_values,),
#     "blip_vision_encoder.onnx",
#     input_names=["pixel_values"],
#     output_names=["encoder_outputs"],
#     dynamic_axes={"pixel_values": {0: "batch_size"}, "encoder_outputs": {0: "batch_size"}},
#     opset_version=13
# )

# print("Exported to blip_vision_encoder.onnx")



from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to("cpu")

image = Image.open("img.jpg").convert("RGB")
inputs = processor(images=image, return_tensors="pt").to("cpu")

output = model.generate(**inputs)
caption = processor.tokenizer.decode(output[0], skip_special_tokens=True)

print("Caption:", caption)

















import torch
from torch import nn
from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image

# Load model and processor
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model.eval()

# Load image and text
image = Image.open("img.jpg").convert("RGB")
text = "a photo of"

inputs = processor(images=image, text=text, return_tensors="pt")
input_ids = inputs["input_ids"]
attention_mask = inputs["attention_mask"]
pixel_values = inputs["pixel_values"]

# Wrap model in a class that uses only positional arguments
class WrappedBLIP(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.model = model

    def forward(self, input_ids, attention_mask, pixel_values):
        return self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            pixel_values=pixel_values
        ).logits

wrapped_model = WrappedBLIP(model)

# Export to ONNX
torch.onnx.export(
    wrapped_model,
    args=(input_ids, attention_mask, pixel_values),
    f="blip_caption.onnx",
    input_names=["input_ids", "attention_mask", "pixel_values"],
    output_names=["output"],
    dynamic_axes={
        "input_ids": {0: "batch", 1: "seq"},
        "attention_mask": {0: "batch", 1: "seq"},
        "pixel_values": {0: "batch"}
    },
    opset_version=13
)

print("âœ… Exported to ONNX successfully.")

